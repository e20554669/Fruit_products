# -*- coding: utf-8 -*-
"""
å°ç£æ°£è±¡è³‡æ–™æ“·å–ç¨‹å¼ - API ç‰ˆæœ¬ï¼ˆæ¨è–¦ï¼‰
ä½¿ç”¨ CODiS API ç›´æ¥ç²å–è³‡æ–™ï¼Œä¸éœ€è¦ Selenium
æ•´åˆé¢±é¢¨è­¦å ±è³‡æ–™
è¼¸å‡ºé€æ™‚ (hourly) å’Œæ¯æ—¥ (daily) å…©ç¨®æ ¼å¼
"""

import pandas as pd
from datetime import datetime, timedelta
import requests
import json
import warnings
from urllib3.exceptions import InsecureRequestWarning
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
import os

warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# =============================
# åƒæ•¸è¨­å®š
# =============================
START_DATE = datetime(2020, 1, 1)
END_DATE = datetime(2025, 10, 22)
OUTPUT_DIR = "codis_data"
OUTPUT_CSV_HOURLY = "taiwan_weather_hourly.csv"  # é€æ™‚è³‡æ–™
OUTPUT_CSV_DAILY = "taiwan_weather_daily.csv"    # æ¯æ—¥è³‡æ–™

os.makedirs(OUTPUT_DIR, exist_ok=True)

# å…¨å° 22 ç¸£å¸‚æ¸¬ç«™è³‡æ–™
STATIONS = [
    {"city_id": "TPE", "city_name": "å°åŒ—å¸‚", "station_id": "466920"},
    {"city_id": "NTP", "city_name": "æ–°åŒ—å¸‚", "station_id": "466880"},
    {"city_id": "TYN", "city_name": "æ¡ƒåœ’å¸‚", "station_id": "467050"},
    {"city_id": "HSC", "city_name": "æ–°ç«¹å¸‚", "station_id": "467080"},
    {"city_id": "HSQ", "city_name": "æ–°ç«¹ç¸£", "station_id": "467571"},
    {"city_id": "MLI", "city_name": "è‹—æ —ç¸£", "station_id": "467300"},
    {"city_id": "TXG", "city_name": "å°ä¸­å¸‚", "station_id": "467490"},
    {"city_id": "CHA", "city_name": "å½°åŒ–ç¸£", "station_id": "467530"},
    {"city_id": "NTO", "city_name": "å—æŠ•ç¸£", "station_id": "467650"},
    {"city_id": "YUN", "city_name": "é›²æ—ç¸£", "station_id": "467550"},
    {"city_id": "CYI", "city_name": "å˜‰ç¾©å¸‚", "station_id": "467410"},
    {"city_id": "CYQ", "city_name": "å˜‰ç¾©ç¸£", "station_id": "467480"},
    {"city_id": "TNN", "city_name": "å°å—å¸‚", "station_id": "467420"},
    {"city_id": "KHH", "city_name": "é«˜é›„å¸‚", "station_id": "467440"},
    {"city_id": "PIF", "city_name": "å±æ±ç¸£", "station_id": "467590"},
    {"city_id": "ILA", "city_name": "å®œè˜­ç¸£", "station_id": "467060"},
    {"city_id": "HUA", "city_name": "èŠ±è“®ç¸£", "station_id": "466990"},
    {"city_id": "TTT", "city_name": "å°æ±ç¸£", "station_id": "467540"},
    {"city_id": "PEN", "city_name": "æ¾æ¹–ç¸£", "station_id": "467350"},
    {"city_id": "KIN", "city_name": "é‡‘é–€ç¸£", "station_id": "467110"},
    {"city_id": "LIE", "city_name": "é€£æ±Ÿç¸£", "station_id": "467990"},
    {"city_id": "KEE", "city_name": "åŸºéš†å¸‚", "station_id": "466940"},
]

# =============================
# é¢±é¢¨è­¦å ±è™•ç†
# =============================
class TyphoonWarningFetcher:
    """ä¸­å¤®æ°£è±¡ç½²é¢±é¢¨è­¦å ±è³‡æ–™æ“·å–å™¨"""
    
    def __init__(self):
        self.typhoon_api_url = "https://rdc28.cwa.gov.tw/TDB/public/warning_typhoon_list/get_warning_typhoon"
        self.typhoon_main_url = "https://rdc28.cwa.gov.tw/TDB/public/warning_typhoon_list/"
    
    def create_scraper_session(self):
        session = requests.Session()
        retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        return session
    
    def fetch_typhoon_warnings_for_year(self, year):
        """æ“·å–æŒ‡å®šå¹´ä»½çš„é¢±é¢¨è­¦å ±è³‡æ–™"""
        print(f"[é¢±é¢¨è­¦å ±] æŸ¥è©¢ {year} å¹´è³‡æ–™...")
        session = self.create_scraper_session()
        try:
            response = session.get(self.typhoon_main_url, timeout=30, verify=False)
            if response.status_code != 200:
                return []
            
            post_data = {"year": str(year)}
            session.headers.update({
                "Referer": self.typhoon_main_url,
                "X-Requested-With": "XMLHttpRequest",
                "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            })
            
            response = session.post(self.typhoon_api_url, data=post_data, timeout=30, verify=False)
            if response.status_code == 200:
                response_text = response.text.strip()
                if response_text.startswith("\ufeff"):
                    response_text = response_text[1:]
                try:
                    data = json.loads(response_text)
                    if isinstance(data, list) and data:
                        print(f"[é¢±é¢¨è­¦å ±] {year} å¹´æ‰¾åˆ° {len(data)} å€‹é¢±é¢¨è­¦å ±")
                        return data
                except:
                    return []
        except:
            return []
        return []
    
    def parse_typhoon_data_to_dates(self, typhoon_warnings):
        """å°‡é¢±é¢¨è­¦å ±è³‡æ–™è§£æç‚ºæ—¥æœŸå°æ‡‰è¡¨"""
        date_warnings = {}
        for warning in typhoon_warnings:
            typhoon_name = warning.get("cht_name", "")
            sea_start = warning.get("sea_start_datetime", "")
            sea_end = warning.get("sea_end_datetime", "")
            
            if sea_start and sea_end:
                try:
                    start_dt = datetime.strptime(sea_start, "%Y-%m-%d %H:%M:%S")
                    end_dt = datetime.strptime(sea_end, "%Y-%m-%d %H:%M:%S")
                    current_date = start_dt.date()
                    end_date = end_dt.date()
                    
                    while current_date <= end_date:
                        date_str = current_date.strftime("%Y-%m-%d")
                        if date_str not in date_warnings:
                            date_warnings[date_str] = []
                        if typhoon_name not in date_warnings[date_str]:
                            date_warnings[date_str].append(typhoon_name)
                        current_date += timedelta(days=1)
                except:
                    continue
        return date_warnings
    
    def fetch_all_warnings(self, start_year, end_year):
        """æ“·å–æŒ‡å®šå¹´ä»½ç¯„åœå…§çš„æ‰€æœ‰é¢±é¢¨è­¦å ±"""
        all_warnings = []
        for year in range(start_year, end_year + 1):
            warnings = self.fetch_typhoon_warnings_for_year(year)
            all_warnings.extend(warnings)
        date_warnings = self.parse_typhoon_data_to_dates(all_warnings)
        print(f"[é¢±é¢¨è­¦å ±] ç¸½è¨ˆæ‰¾åˆ° {len(date_warnings)} å€‹æœ‰é¢±é¢¨è­¦å ±çš„æ—¥æœŸ\n")
        return date_warnings

# =============================
# CODiS API çˆ¬èŸ²ï¼ˆä¸éœ€è¦ Seleniumï¼‰
# =============================
class CODiSAPICrawler:
    """ä½¿ç”¨ CODiS API ç›´æ¥ç²å–è³‡æ–™"""
    
    def __init__(self):
        self.api_url = "https://codis.cwa.gov.tw/api/station"
        self.session = requests.Session()
        
        # è¨­å®šé‡è©¦ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è¨­å®š Headers
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
            "Referer": "https://codis.cwa.gov.tw/StationData",
            "Origin": "https://codis.cwa.gov.tw"
        })
    
    def get_station_type(self, station_id):
        """æ ¹æ“šæ¸¬ç«™ä»£è™Ÿåˆ¤æ–·é¡å‹"""
        prefix = station_id[:2]
        if prefix == "46":
            return "cwb"  # ç½²å±¬æœ‰äººç«™
        elif prefix == "C1":
            return "auto_C1"  # è‡ªå‹•é›¨é‡ç«™
        elif prefix == "C0":
            return "auto_C0"  # è‡ªå‹•æ°£è±¡ç«™
        else:
            return "agr"  # è¾²æ¥­ç«™
    
    def parse_hourly_data(self, data_dict):
        """è§£æé€æ™‚è³‡æ–™"""
        parsed = {}
        for key, value in data_dict.items():
            if key == 'DataTime':
                parsed['DataTime'] = value
                continue
            
            # è™•ç†å·¢ç‹€çµæ§‹ (ä¾‹å¦‚: {'Instantaneous': 999.8})
            if isinstance(value, dict):
                # å–ç¬¬ä¸€å€‹å€¼ï¼ˆé€šå¸¸æ˜¯ Instantaneousï¼‰
                if value:
                    first_key = list(value.keys())[0]
                    raw_value = value[first_key]
                    
                    # è™•ç†ç‰¹æ®Šä»£ç¢¼ï¼šè² å€¼é€šå¸¸ä»£è¡¨ç„¡æ•ˆè³‡æ–™
                    if raw_value is not None and isinstance(raw_value, (int, float)):
                        if raw_value < 0:
                            parsed[key] = None  # å°‡è² å€¼è½‰ç‚º None (ç©ºå€¼)
                        else:
                            parsed[key] = raw_value
                    else:
                        parsed[key] = raw_value
                else:
                    parsed[key] = None
            else:
                # è™•ç†éå·¢ç‹€çµæ§‹çš„ç‰¹æ®Šä»£ç¢¼
                if value is not None and isinstance(value, (int, float)):
                    if value < 0:
                        parsed[key] = None
                    else:
                        parsed[key] = value
                else:
                    parsed[key] = value
        
        return parsed
    
    def fetch_weather_data(self, station_id, target_date):
        """ç²å–æŒ‡å®šæ¸¬ç«™çš„æŒ‡å®šæ—¥æœŸè³‡æ–™"""
        
        # æº–å‚™ API åƒæ•¸
        date_str = target_date.strftime("%Y-%m-%d")
        date = f"{date_str}T00:00:00.000+08:00"
        start = f"{date_str}T00:00:00"
        end = f"{date_str}T23:59:59"
        
        stn_type = self.get_station_type(station_id)
        
        data = {
            "type": "report_date",
            "more": "",
            "item": "",
            "stn_type": stn_type,
            "date": date,
            "start": start,
            "end": end,
            "stn_ID": station_id,
        }
        
        try:
            # ç¬¬ä¸€æ¬¡è«‹æ±‚ï¼ˆå»ºç«‹ sessionï¼‰- é—œé–‰ SSL é©—è­‰
            self.session.get(self.api_url, verify=False, timeout=10)
            
            # POST è«‹æ±‚ç²å–è³‡æ–™ - é—œé–‰ SSL é©—è­‰
            response = self.session.post(self.api_url, data=data, verify=False, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                
                # æª¢æŸ¥å›æ‡‰çµæ§‹
                if 'code' in result and result['code'] != 200:
                    # API å›å‚³éŒ¯èª¤
                    return None
                
                if 'data' in result and len(result['data']) > 0:
                    # ä½¿ç”¨æ­£ç¢ºçš„ key: 'dts'
                    hourly_data = result['data'][0].get('dts', [])
                    
                    if not hourly_data:
                        return None
                    
                    # è§£ææ¯å°æ™‚è³‡æ–™
                    parsed_records = []
                    for hour_dict in hourly_data:
                        parsed_record = self.parse_hourly_data(hour_dict)
                        parsed_records.append(parsed_record)
                    
                    # è½‰æ›ç‚º DataFrame
                    df = pd.DataFrame(parsed_records)
                    
                    return df
                else:
                    return None
            else:
                return None
                
        except Exception as e:
            return None

# =============================
# è³‡æ–™è™•ç†å‡½æ•¸
# =============================
def create_daily_summary(hourly_df):
    """å°‡é€æ™‚è³‡æ–™å½™ç¸½ç‚ºæ¯æ—¥è³‡æ–™"""
    
    # ç¢ºä¿ DataTime æ˜¯ datetime æ ¼å¼
    hourly_df['DataTime'] = pd.to_datetime(hourly_df['DataTime'])
    
    # æå–æ—¥æœŸ
    hourly_df['Date'] = hourly_df['DataTime'].dt.date
    
    # å®šç¾©ä¸åŒé¡å‹çš„æ¬„ä½
    # 1. éœ€è¦è¨ˆç®—å¹³å‡å€¼çš„æ¬„ä½ï¼ˆç¬æ™‚ç‹€æ…‹å€¼ï¼‰
    mean_cols = [
        'StationPressure', 'SeaLevelPressure', 'AirTemperature', 
        'DewPointTemperature', 'RelativeHumidity', 'WindSpeed',
        'Visibility', 'UVIndex', 'TotalCloudAmount',
        'SoilTemperatureAt0cm', 'SoilTemperatureAt5cm', 'SoilTemperatureAt10cm',
        'SoilTemperatureAt20cm', 'SoilTemperatureAt30cm', 'SoilTemperatureAt50cm',
        'SoilTemperatureAt100cm', 'GlobalSolarRadiation'
    ]
    
    # 2. éœ€è¦ç´¯ç©åŠ ç¸½çš„æ¬„ä½ï¼ˆç´¯ç©é‡ï¼‰
    sum_cols = [
        'Precipitation',           # é™æ°´é‡ï¼šç´¯ç©ä¸€å¤©çš„ç¸½é›¨é‡
        'PrecipitationDuration',   # é™æ°´å»¶æ™‚ï¼šç´¯ç©ä¸€å¤©ä¸‹é›¨çš„ç¸½æ™‚æ•¸
        'SunshineDuration'         # æ—¥ç…§æ™‚æ•¸ï¼šç´¯ç©ä¸€å¤©çš„æ—¥ç…§ç¸½æ™‚æ•¸
    ]
    
    # 3. éœ€è¦å–æœ€å¤§å€¼çš„æ¬„ä½
    max_cols = [
        'PeakGust',  # æœ€å¤§é™£é¢¨ï¼šç•¶å¤©æœ€å¼·çš„é™£é¢¨
        'typhoon'    # é¢±é¢¨è­¦å ±ï¼šç•¶å¤©æ˜¯å¦æœ‰é¢±é¢¨ï¼ˆ1=æœ‰ï¼Œ0=ç„¡ï¼‰
    ]
    
    # å»ºç«‹åˆ†çµ„æ¬„ä½
    group_cols = ['Date', 'city_id', 'city_name', 'station_id']
    
    # å»ºç«‹å½™ç¸½è¦å‰‡
    agg_dict = {}
    
    # å¹³å‡å€¼æ¬„ä½
    for col in mean_cols:
        if col in hourly_df.columns:
            agg_dict[col] = 'mean'
    
    # ç´¯ç©å€¼æ¬„ä½
    for col in sum_cols:
        if col in hourly_df.columns:
            agg_dict[col] = 'sum'
    
    # æœ€å¤§å€¼æ¬„ä½
    for col in max_cols:
        if col in hourly_df.columns:
            agg_dict[col] = 'max'
    
    # é¢¨å‘éœ€è¦ç‰¹æ®Šè™•ç†ï¼ˆå‘é‡å¹³å‡ï¼‰
    if 'WindDirection' in hourly_df.columns:
        # å–çœ¾æ•¸ï¼ˆæœ€å¸¸å‡ºç¾çš„é¢¨å‘ï¼‰
        agg_dict['WindDirection'] = lambda x: x.mode()[0] if len(x.mode()) > 0 else x.mean()
    
    # é¢±é¢¨åç¨±å–ç¬¬ä¸€å€‹éç©ºå€¼
    if 'typhoon_name' in hourly_df.columns:
        agg_dict['typhoon_name'] = lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else ''
    
    # åŸ·è¡Œåˆ†çµ„å½™ç¸½
    daily_df = hourly_df.groupby(group_cols).agg(agg_dict).reset_index()
    
    # å°‡ Date è½‰å›å­—ä¸²æ ¼å¼
    daily_df['Date'] = daily_df['Date'].astype(str)
    
    # å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œ 1 ä½
    numeric_cols = daily_df.select_dtypes(include=['float64']).columns
    for col in numeric_cols:
        if col not in ['typhoon']:
            daily_df[col] = daily_df[col].round(1)
    
    return daily_df

# =============================
# ä¸»æµç¨‹
# =============================
def main():
    print("\n" + "="*70)
    print("ğŸŒ¤ï¸  å°ç£æ°£è±¡è³‡æ–™æ“·å–ç¨‹å¼ï¼ˆCODiS API + é¢±é¢¨è­¦å ±ï¼‰")
    print("="*70 + "\n")
    
    # æ­¥é©Ÿ1ï¼šæ“·å–é¢±é¢¨è­¦å ±
    print("[æ­¥é©Ÿ 1] æ“·å–é¢±é¢¨è­¦å ±è³‡æ–™")
    print("-"*70)
    typhoon_fetcher = TyphoonWarningFetcher()
    typhoon_date_dict = typhoon_fetcher.fetch_all_warnings(START_DATE.year, END_DATE.year)
    
    # æ­¥é©Ÿ2ï¼šåˆå§‹åŒ– API çˆ¬èŸ²
    print("[æ­¥é©Ÿ 2] åˆå§‹åŒ– API çˆ¬èŸ²")
    print("-"*70)
    crawler = CODiSAPICrawler()
    print("âœ… API çˆ¬èŸ²åˆå§‹åŒ–å®Œæˆ\n")
    
    # æ­¥é©Ÿ3ï¼šçˆ¬å–è³‡æ–™
    print("[æ­¥é©Ÿ 3] çˆ¬å–æ°£è±¡è³‡æ–™")
    print("-"*70)
    
    all_data = []
    success_count = 0
    fail_count = 0
    
    # ç”Ÿæˆæ—¥æœŸç¯„åœ
    current_date = START_DATE
    dates = []
    while current_date <= END_DATE:
        dates.append(current_date)
        current_date += timedelta(days=1)
    
    # çˆ¬å–æ‰€æœ‰æ¸¬ç«™å’Œæ—¥æœŸ
    total_tasks = len(STATIONS) * len(dates)
    
    with tqdm(total=total_tasks, desc="ä¸‹è¼‰é€²åº¦") as pbar:
        for station in STATIONS:
            for date in dates:
                pbar.set_description(f"{station['city_name']} {date.strftime('%Y-%m-%d')}")
                
                # ç²å–è³‡æ–™
                df = crawler.fetch_weather_data(station['station_id'], date)
                
                if df is not None and not df.empty:
                    # åŠ å…¥åŸå¸‚è³‡è¨Š
                    df['city_id'] = station['city_id']
                    df['city_name'] = station['city_name']
                    df['station_id'] = station['station_id']
                    
                    # åŠ å…¥é¢±é¢¨è³‡è¨Š
                    date_str = date.strftime("%Y-%m-%d")
                    if date_str in typhoon_date_dict:
                        df['typhoon'] = 1
                        df['typhoon_name'] = ', '.join(typhoon_date_dict[date_str])
                    else:
                        df['typhoon'] = 0
                        df['typhoon_name'] = ''
                    
                    all_data.append(df)
                    success_count += 1
                else:
                    fail_count += 1
                
                pbar.update(1)
    
    # æ­¥é©Ÿ4ï¼šè³‡æ–™å½™ç¸½èˆ‡è¼¸å‡º
    print("\n[æ­¥é©Ÿ 4] è³‡æ–™å½™ç¸½èˆ‡è¼¸å‡º")
    print("-"*70)
    
    if all_data:
        # åˆä½µæ‰€æœ‰é€æ™‚è³‡æ–™
        hourly_df = pd.concat(all_data, ignore_index=True)
        
        # è³‡æ–™å“è³ªçµ±è¨ˆ
        total_records = len(hourly_df)
        
        print(f"ğŸ“Š é€æ™‚è³‡æ–™å“è³ªæª¢æŸ¥:")
        numeric_cols = hourly_df.select_dtypes(include=['float64', 'int64']).columns
        null_summary = []
        for col in numeric_cols:
            if col not in ['typhoon']:  # æ’é™¤ typhoon æ¬„ä½
                null_count = hourly_df[col].isna().sum()
                null_pct = (null_count / total_records) * 100
                if null_count > 0:
                    null_summary.append(f"   - {col}: {null_count} ç­†ç©ºå€¼ ({null_pct:.1f}%)")
        
        if null_summary:
            for line in null_summary[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                print(line)
            if len(null_summary) > 5:
                print(f"   ... é‚„æœ‰ {len(null_summary) - 5} å€‹æ¬„ä½æœ‰ç©ºå€¼")
        else:
            print("   âœ… æ‰€æœ‰æ¬„ä½éƒ½ç„¡ç©ºå€¼")
        
        # è¼¸å‡ºé€æ™‚è³‡æ–™
        hourly_output_path = os.path.join(OUTPUT_DIR, OUTPUT_CSV_HOURLY)
        hourly_df.to_csv(hourly_output_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ… å·²è¼¸å‡ºé€æ™‚è³‡æ–™ï¼š{hourly_output_path}")
        print(f"   ç¸½ç­†æ•¸ï¼š{len(hourly_df):,} ç­†ï¼ˆæ¯å°æ™‚ä¸€ç­†ï¼‰")
        print(f"   æ™‚é–“ç¯„åœï¼š{hourly_df['DataTime'].min()} ~ {hourly_df['DataTime'].max()}")
        
        # ç”¢ç”Ÿæ¯æ—¥å½™ç¸½è³‡æ–™
        print(f"\nğŸ“Š ç”¢ç”Ÿæ¯æ—¥å½™ç¸½è³‡æ–™...")
        daily_df = create_daily_summary(hourly_df)
        
        # è¼¸å‡ºæ¯æ—¥è³‡æ–™
        daily_output_path = os.path.join(OUTPUT_DIR, OUTPUT_CSV_DAILY)
        daily_df.to_csv(daily_output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… å·²è¼¸å‡ºæ¯æ—¥è³‡æ–™ï¼š{daily_output_path}")
        print(f"   ç¸½ç­†æ•¸ï¼š{len(daily_df):,} ç­†ï¼ˆæ¯æ—¥å½™ç¸½ï¼‰")
        print(f"   æ—¥æœŸç¯„åœï¼š{daily_df['Date'].min()} ~ {daily_df['Date'].max()}")
        
        print(f"\nğŸ“‹ è³‡æ–™æ¬„ä½:")
        print(f"   é€æ™‚è³‡æ–™: {len(hourly_df.columns)} å€‹æ¬„ä½")
        print(f"   æ¯æ—¥è³‡æ–™: {len(daily_df.columns)} å€‹æ¬„ä½")
        
        print(f"\nğŸ’¡ æ¯æ—¥è³‡æ–™å½™ç¸½æ–¹å¼:")
        print(f"   ğŸ“Š å¹³å‡å€¼ (Mean): æ°£æº«ã€æ°£å£“ã€æ¿•åº¦ã€é¢¨é€Ÿã€èƒ½è¦‹åº¦ã€åœŸå£¤æº«åº¦ç­‰")
        print(f"   â• ç´¯ç©å€¼ (Sum): é™æ°´é‡ã€é™æ°´å»¶æ™‚ã€æ—¥ç…§æ™‚æ•¸")
        print(f"   ğŸ“ˆ æœ€å¤§å€¼ (Max): æœ€å¤§é™£é¢¨ã€é¢±é¢¨è­¦å ±")
        print(f"\nğŸ’¡ å…¶ä»–èªªæ˜:")
        print(f"   - é€æ™‚è³‡æ–™: åŸå§‹è³‡æ–™ï¼Œæ¯å°æ™‚ä¸€ç­†è¨˜éŒ„")
        print(f"   - ç©ºå€¼ (NaN): è©²æ™‚æ®µæ¸¬ç«™æœªæä¾›è³‡æ–™æˆ–å„€å™¨æ•…éšœ")
        print(f"   - åŸå§‹è³‡æ–™ä¸­çš„è² å€¼å·²è‡ªå‹•è½‰æ›ç‚ºç©ºå€¼")
    else:
        print("âš ï¸ æ²’æœ‰æˆåŠŸä¸‹è¼‰ä»»ä½•è³‡æ–™")
    
    # çµ±è¨ˆ
    print("\n" + "="*70)
    print("ğŸ“Š åŸ·è¡Œçµæœ")
    print("="*70)
    print(f"âœ… æˆåŠŸ: {success_count} ç­†")
    print(f"âŒ å¤±æ•—: {fail_count} ç­†")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    print(f"ğŸ“„ é€æ™‚è³‡æ–™: {OUTPUT_CSV_HOURLY} (æ¯å°æ™‚ä¸€ç­†ï¼Œå…± 24 ç­†/å¤©)")
    print(f"ğŸ“„ æ¯æ—¥è³‡æ–™: {OUTPUT_CSV_DAILY} (æ¯æ—¥å½™ç¸½ï¼Œå…± 1 ç­†/å¤©)")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
